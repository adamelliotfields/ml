{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/ml/blob/main/notebooks/autograd.ipynb)\n",
    "[![Render nbviewer](https://img.shields.io/badge/render-nbviewer-f37726)](https://nbviewer.org/github/adamelliotfields/ml/blob/main/notebooks/autograd.ipynb)\n",
    "\n",
    "This is a neural network I put together from scratch.\n",
    "\n",
    "The key feature is _automatic differentiation_, which we get from the [Autograd](https://github.com/HIPS/autograd) package. This enables us to compute the gradients of our loss function.\n",
    "\n",
    "The interface is the same as NumPy with the additional `grad` function. The benefit of Autograd over the newer JAX (and PyTorch) is that it's pure Python, so it can be installed anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "from autograd import grad\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network classifies Iris flowers using the famous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) into three species: setosa, versicolor, and virginica. Species is the target variable.\n",
    "\n",
    "The features (independent variables) are 4 measurements of the flowers: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "The architecture is a simple feedforward neural network also known as a multilayer perceptron (MLP). It has 4 input neurons, 3 output neurons, and 1 hidden layer with 8 neurons. Unlike more advanced architectures, a feedforward net has no loops (recurrent connections) or skips (residual connections); it's just a sequence of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species_setosa</th>\n",
       "      <th>species_versicolor</th>\n",
       "      <th>species_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  species_setosa  \\\n",
       "73            6.1          2.8           4.7          1.2               0   \n",
       "18            5.7          3.8           1.7          0.3               1   \n",
       "118           7.7          2.6           6.9          2.3               0   \n",
       "78            6.0          2.9           4.5          1.5               0   \n",
       "76            6.8          2.8           4.8          1.4               0   \n",
       "\n",
       "     species_versicolor  species_virginica  \n",
       "73                    1                  0  \n",
       "18                    0                  0  \n",
       "118                   0                  1  \n",
       "78                    1                  0  \n",
       "76                    1                  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "# play around with these!\n",
    "LAYER_SIZES = [4, 8, 3]\n",
    "LEARNING_RATE = 1e-2  # 0.01\n",
    "EPOCHS = 50\n",
    "\n",
    "# column names\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# load and convert to dataframe\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris[\"data\"], columns=columns)\n",
    "\n",
    "# one-hot encode species\n",
    "species = iris[\"target_names\"]\n",
    "iris_df[\"species\"] = species[iris[\"target\"]]\n",
    "iris_df = pd.get_dummies(iris_df, columns=[\"species\"])\n",
    "\n",
    "# train-test split\n",
    "X = iris_df[columns].values\n",
    "y = iris_df.iloc[:, -3:].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "iris_df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to initialize the parameters with random numbers to start (`init_params`).\n",
    "\n",
    "The activation function (`relu`) simply returns the input if it's positive, and 0 otherwise.\n",
    "\n",
    "The `logsumexp` function is a _numerically stable_ way to compute the log of the sum of the exponentials for the `softmax` activation function in the output layer. When dealing with really small or really big numbers, we can run into underflow or overflow errors. This means that tiny numbers are rounded to zero and huge numbers are rounded to infinity. In machine learning, these rounding errors could lead to inaccurate predictions.\n",
    "\n",
    "The `predict` function returns _logits_, which we convert to probabilities in the output layer. Logits are raw (not normalized) numbers generated by the model. They are the input to the softmax function, which returns a distribution of probabilities over the classes.\n",
    "\n",
    "The `loss` function is _category cross-entropy_ (CCE). This is a measure of the dissimilarity between the true distribution and the predicted distribution. The loss function is _minimized_ when the predicted distribution is close to the true distribution. Worth mentioning that in PyTorch, CCE includes softmax, so raw logits should be passed.\n",
    "\n",
    "> Note that JAX provides `logsumexp`; Autograd does not. We cannot use SciPy's because it wouldn't be differentiable, so we have to write our own with `autograd.numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "def init_params(sizes):\n",
    "    params = []  # weight/bias pairs\n",
    "    for m, n in zip(sizes[:-1], sizes[1:]):\n",
    "        weight = 1e-2 * np.random.randn(n, m)\n",
    "        bias = 1e-2 * np.random.randn(n)\n",
    "        params.append((weight, bias))\n",
    "    return params\n",
    "\n",
    "\n",
    "# relu activation in hidden layers\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# logsumexp for numerical stability\n",
    "# need to reshape because we can't use `keepdims` for `max` in autograd.numpy\n",
    "def logsumexp(logits):\n",
    "    max_logits = np.max(logits, axis=1)\n",
    "    sum_exp = np.sum(np.exp(logits - max_logits[:, None]), axis=1)\n",
    "    log_sum_exp = max_logits + np.log(sum_exp)\n",
    "    return log_sum_exp[:, None]\n",
    "\n",
    "\n",
    "# softmax activation in output layer\n",
    "def softmax(logits):\n",
    "    shifted_logits = logits - logsumexp(logits)\n",
    "    return np.exp(shifted_logits)\n",
    "\n",
    "\n",
    "# predict function\n",
    "def predict(params, example):\n",
    "    # ensure example is 2D, even if single example\n",
    "    if example.ndim == 1:\n",
    "        example = np.expand_dims(example, axis=0)  # same as `[None, :]`\n",
    "\n",
    "    # loop over all layers except output\n",
    "    activations = example\n",
    "    for weight, bias in params[:-1]:\n",
    "        # dot product of the weights and the activations of the previous layer plus bias\n",
    "        # (transpose to ensure dimensions align)\n",
    "        outputs = np.dot(weight, activations.T).T + bias\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    # extract the parameters for the final layer\n",
    "    final_weight, final_bias = params[-1]\n",
    "    logits = np.dot(final_weight, activations.T).T + final_bias\n",
    "\n",
    "    # subtract the log of the sum of the exponentiated logits for numerical stability\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "\n",
    "# accuracy function\n",
    "def accuracy(params, example, target):\n",
    "    logits = predict(params, example)\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    targets = np.argmax(target, axis=1)\n",
    "    return np.mean(preds == targets)\n",
    "\n",
    "\n",
    "# loss function\n",
    "def loss(params, example, target):\n",
    "    logits = predict(params, example)\n",
    "    probs = softmax(logits)\n",
    "    return -np.mean(np.sum(target * np.log(probs), axis=1))  # categorical cross-entropy\n",
    "\n",
    "\n",
    "# single step of gradient descent for training loop\n",
    "def update(params, x, y, learning_rate):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    new_params = []\n",
    "    for (weight, bias), (weight_grad, bias_grad) in zip(params, grads):\n",
    "        new_weight = weight - learning_rate * weight_grad\n",
    "        new_bias = bias - learning_rate * bias_grad\n",
    "        new_params.append((new_weight, new_bias))\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 50/50 [00:07<00:00,  6.30it/s, accuracy=0.967, loss=0.109]\n"
     ]
    }
   ],
   "source": [
    "# init network\n",
    "model = init_params(LAYER_SIZES)\n",
    "\n",
    "# train!\n",
    "prog_bar = tqdm(range(EPOCHS))\n",
    "for epoch in prog_bar:\n",
    "    for example, target in zip(X_train, y_train):\n",
    "        example_2d = np.expand_dims(example, axis=0)\n",
    "        target_2d = np.expand_dims(target, axis=0)\n",
    "        model = update(model, example_2d, target_2d, LEARNING_RATE)\n",
    "    acc = accuracy(model, X_test, y_test)\n",
    "    lss = loss(model, X_test, y_test)\n",
    "    prog_bar.set_postfix(accuracy=f\"{acc:.3f}\", loss=f\"{lss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model looks like this:\n",
    "\n",
    "```json\n",
    "[\n",
    "  [\n",
    "    [\n",
    "      [-5.34416454e-03, -1.49505387e-02, -7.89258329e-03, 7.43711284e-03],\n",
    "      [-1.52876246e-01, -4.83584651e-01, 1.25840621e+00, 7.66275274e-01],\n",
    "      [-1.12059472e+00, -1.17667945e+00, 1.99320102e+00, 1.56054733e+00],\n",
    "      [-4.03155807e-03, 6.01119070e-03, -2.81482396e-03, -3.71979647e-03],\n",
    "      [ 8.77257997e-03, -5.69623799e-03, -2.75949741e-02, 3.27996260e-03],\n",
    "      [-3.95516452e-03, -2.89136856e-03, 4.52936327e-03, -1.66060908e-03],\n",
    "      [ 2.14938830e-03, -2.02231493e-02, -9.43056808e-03, 1.40395874e-02],\n",
    "      [-1.85508045e-04, -1.67350462e-02, -1.07253183e-02, -9.92586179e-03]\n",
    "    ],\n",
    "    [1.02347683e-03, -1.31112511e-01, -5.66512209e-01, -1.49189067e-04, 3.99712755e-03, -2.59028645e-03, -5.74709208e-03, -4.21498220e-03]\n",
    "  ], [\n",
    "    [\n",
    "      [3.39820964e-03, -1.55629341e+00, -6.26243719e-01, -1.13993816e-02, -6.76569892e-03, 7.73140856e-03, -8.01827843e-03, 1 38401572e-02],\n",
    "      [1.40520531e-02, 8.38914200e-01, -1.04577923e+00, 7.19771668e-04, -5.42906466e-03,  9.23162608e-03, 1.70660495e-02, 8.73589416e-03],\n",
    "      [9.14434574e-05, 7.27573270e-01, 1.67738038e+00, -1.22781017e-02, 4.87043802e-03, -9.14690931e-03, 6.20548216e-03, -1.60937377e-03]\n",
    "    ],\n",
    "    [2.75982171, -0.11198307, -2.66414385]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "So, `model[0][0]` is a 2D array of shape `(8, 4)`, meaning there are 8 arrays of length 4. There are 4 input features corresponding to the 4 measurements of the iris flowers. There are 8 neurons in the first \"hidden\" layer.\n",
    "\n",
    "The next item is a 1D array of shape `(8,)` (length of 8). These are the biases of the 8 neurons in the first layer.\n",
    "\n",
    "Biases are like the intercept in a linear equation. If you were predicting the price of a house, in a linear model the bias would be the baseline price before taking into account the number of bedrooms, bathrooms, etc.\n",
    "\n",
    "The output layer has 3 neurons corresponding to the 3 species of iris flowers. The weights are a 2D array of shape `(3, 8)`: 3 arrays of length 8, or 3 neurons with 8 inputs each. The biases are a 1D array of shape `(3,)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
